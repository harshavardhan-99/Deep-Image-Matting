{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from DataLoader.ipynb\n",
      "importing Jupyter notebook from ConvNet.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "# import Dataloader\n",
    "import torch\n",
    "import time\n",
    "from DataLoader import load_dataset\n",
    "from ConvNet import get_model\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_stage1(alpha, trimap, pred_alpha, fg, bg, img, cuda):\n",
    "    wi = torch.zeros(trimap.shape)\n",
    "    wi[trimap == 128] = 1.\n",
    "    t_wi = wi\n",
    "    t3_wi = torch.cat((wi, wi, wi), 1)\n",
    "    if cuda:\n",
    "        t3_wi = t3_wi.cuda()\n",
    "        t_wi = wi.cuda()\n",
    "\n",
    "    unknown_region_size = t_wi.sum()\n",
    "\n",
    "    # alpha loss\n",
    "    alpha = alpha / 255.\n",
    "    alpha_loss = torch.sqrt((pred_alpha - alpha)**2 + 1e-12)\n",
    "    alpha_loss = (alpha_loss * t_wi).sum() / (unknown_region_size + 1.)\n",
    "\n",
    "    # composite rgb loss\n",
    "    pred_alpha_3 = torch.cat((pred_alpha, pred_alpha, pred_alpha), 1)\n",
    "#     print(fg.shape, bg.shape, pred_alpha_3.shape)\n",
    "    comp = pred_alpha_3 * fg + (1. - pred_alpha_3) * bg\n",
    "    comp_loss = torch.sqrt((comp - img) ** 2 + 1e-12) / 255.\n",
    "    comp_loss = (comp_loss * t3_wi).sum() / (unknown_region_size + 1.) / 3.\n",
    "    return alpha_loss, comp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement stage Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_stage2(alpha, pred_alpha, trimap, cuda):\n",
    "    wi = torch.zeros(trimap.shape)\n",
    "    wi[trimap == 128] = 1.\n",
    "    t_wi = wi\n",
    "    if cuda:\n",
    "        t_wi = wi.cuda()\n",
    "    unknown_region_size = t_wi.sum()\n",
    "    # alpha loss\n",
    "    alpha = alpha / 255.\n",
    "#     print(pred_alpha.size())\n",
    "    alpha_loss = torch.sqrt((pred_alpha - alpha)**2 + 1e-12)\n",
    "    alpha_loss = (alpha_loss * t_wi).sum() / (unknown_region_size + 1.)\n",
    "    \n",
    "    return alpha_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep checking learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lr(optimizer, epoch):\n",
    "    if epoch >= 10:\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_pretrain_vals(model, vgg_dict):\n",
    "    model.load_state_dict(vgg_dict,strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a file list array for data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_list(fg_file, bg_file):\n",
    "    fg_list = []\n",
    "    bg_list = []\n",
    "    with open(fg_file, 'r') as f:\n",
    "        fg_list = f.readlines()\n",
    "    with open(bg_file, 'r') as f:\n",
    "        bg_list = f.readlines()\n",
    "    for i in range(len(fg_list)):\n",
    "        fg_list[i] = fg_list[i].strip('\\n').strip('\\r')\n",
    "    for i in range(len(bg_list)):\n",
    "        bg_list[i] = bg_list[i].strip('\\n').strip('\\r')\n",
    "    arr = []\n",
    "    cnt = 0\n",
    "    for i in range(len(fg_list)):\n",
    "        for j in range(100):\n",
    "            arr.append((fg_list[i], bg_list[cnt], fg_list[i]))\n",
    "            cnt += 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(stage, model, optimizer, dataset, epoch, cuda=False):\n",
    "    model.train()\n",
    "#     for iteration, batch in enumerate(dataset, 1):\n",
    "#         print(iteration)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         print(iteration, epoch)\n",
    "#         print(batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[4].shape)\n",
    "#         img = Variable(batch[0])\n",
    "#         alpha = Variable(batch[1])\n",
    "#         fg = Variable(batch[2])\n",
    "#         bg = Variable(batch[3])\n",
    "#         trimap = Variable(batch[4])\n",
    "\n",
    "#         if cuda:\n",
    "#             img = img.cuda()\n",
    "#             alpha = alpha.cuda()\n",
    "#             fg = fg.cuda()\n",
    "#             bg = bg.cuda()\n",
    "#             trimap = trimap.cuda()\n",
    "\n",
    "#         check_lr(optimizer, epoch)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         pred_alpha = model(torch.cat((img, trimap), 1))\n",
    "\n",
    "#         alpha_loss, comp_loss = loss_stage1(alpha, trimap, pred_alpha, fg, bg, img, cuda)\n",
    "#         loss = alpha_loss*0.5 + comp_loss*0.5\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    print(dataset.__len__())\n",
    "    for iteration in range(dataset.__len__()):\n",
    "#         print(iteration)\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            batch = dataset.__getitem__(iteration)\n",
    "            if batch == None:\n",
    "                continue\n",
    "    #         print(batch[0].shape)\n",
    "    #         print(batch[0])\n",
    "            batch0 = batch[0].reshape(1, batch[0].shape[0], batch[0].shape[1], batch[0].shape[2])\n",
    "            batch1 = batch[1].reshape(1, batch[1].shape[0], batch[1].shape[1], batch[1].shape[2])\n",
    "            batch2 = batch[2].reshape(1, batch[2].shape[0], batch[2].shape[1], batch[2].shape[2])\n",
    "            batch3 = batch[3].reshape(1, batch[3].shape[0], batch[3].shape[1], batch[3].shape[2])\n",
    "            batch4 = batch[4].reshape(1, batch[4].shape[0], batch[4].shape[1], batch[4].shape[2])\n",
    "            img = Variable(batch0)\n",
    "            alpha = Variable(batch1)\n",
    "            fg = Variable(batch2)\n",
    "            bg = Variable(batch3)\n",
    "            trimap = Variable(batch4)\n",
    "    #         print(batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape, batch[4].shape)\n",
    "    #         print(img.shape, alpha.shape, bg.shape, fg.shape, trimap.shape)\n",
    "\n",
    "            if cuda:\n",
    "                img = img.cuda()\n",
    "                alpha = alpha.cuda()\n",
    "                fg = fg.cuda()\n",
    "                bg = bg.cuda()\n",
    "                trimap = trimap.cuda()\n",
    "\n",
    "            check_lr(optimizer, epoch)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred_mattes, pred_alpha = model(torch.cat((img, trimap), 1))\n",
    "\n",
    "    #         alpha_loss, comp_loss = loss_stage1(alpha, trimap, pred_alpha, fg, bg, img, cuda)\n",
    "    #         loss = alpha_loss*0.5 + comp_loss*0.5\n",
    "            wl_weight = 0.5\n",
    "\n",
    "            if stage == 1:\n",
    "                # stage1 loss\n",
    "                alpha_loss, comp_loss = loss_stage1(alpha,trimap,pred_mattes, fg, bg, img , cuda)\n",
    "                loss = alpha_loss * wl_weight + comp_loss * (1. - wl_weight)\n",
    "            elif stage == 2:\n",
    "                # stage2 loss\n",
    "                loss = loss_stage2(alpha, pred_alpha, trimap, cuda)\n",
    "            else:\n",
    "                # stage3 loss = stage1 loss + stage2 loss\n",
    "                alpha_loss, comp_loss = loss_stage1(alpha,trimap,pred_mattes, fg, bg, img , cuda)\n",
    "                loss1 = alpha_loss * wl_weight + comp_loss * (1. - wl_weight)\n",
    "                loss2 = loss_stage2(alpha, pred_alpha, trimap, cuda)\n",
    "                loss = loss1 + loss2\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % 20 == 0:\n",
    "                print(iteration, epoch, loss)\n",
    "        except:\n",
    "            print(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, model):\n",
    "    torch.save(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDNet(\n",
      "  (conv1_1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (deconv6_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (deconv5_1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (deconv4_1): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (deconv3_1): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (deconv2_1): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (deconv1_1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (deconv1): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (refine_conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (refine_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (refine_conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (refine_pred): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "43100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subramanyam.m/Deep-Image-Matting/venv/local/lib/python2.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, tensor(0.3272, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(20, 0, tensor(0.3202, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(40, 0, tensor(0.2016, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(60, 0, tensor(0.1724, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "64\n",
      "(80, 0, tensor(0.2085, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(100, 0, tensor(0.2258, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(120, 0, tensor(0.2896, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(140, 0, tensor(0.2236, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(160, 0, tensor(0.1676, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(180, 0, tensor(0.2071, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "189\n",
      "(200, 0, tensor(0.2143, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(220, 0, tensor(0.2053, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(240, 0, tensor(0.1435, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(260, 0, tensor(0.2092, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(280, 0, tensor(0.1173, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(300, 0, tensor(0.1054, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(320, 0, tensor(0.2042, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(340, 0, tensor(0.1116, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(360, 0, tensor(0.1236, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "364\n",
      "(380, 0, tensor(0.1384, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(400, 0, tensor(0.2067, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(420, 0, tensor(0.1388, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(440, 0, tensor(0.1307, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(460, 0, tensor(0.1292, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(480, 0, tensor(0.0900, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(500, 0, tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(520, 0, tensor(0.1482, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(540, 0, tensor(0.0728, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(560, 0, tensor(0.1336, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(580, 0, tensor(0.1050, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(600, 0, tensor(0.2527, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "613\n",
      "(620, 0, tensor(0.0745, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(640, 0, tensor(0.1085, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(660, 0, tensor(0.1375, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(680, 0, tensor(0.1123, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(700, 0, tensor(0.1343, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(720, 0, tensor(0.0885, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(740, 0, tensor(0.1186, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(760, 0, tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "777\n",
      "(780, 0, tensor(0.1164, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(800, 0, tensor(0.2583, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(820, 0, tensor(0.2109, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(840, 0, tensor(0.2013, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(860, 0, tensor(0.1967, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(880, 0, tensor(0.2662, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(900, 0, tensor(0.2356, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(920, 0, tensor(0.2064, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(940, 0, tensor(0.2399, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(960, 0, tensor(0.1460, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "968\n",
      "(980, 0, tensor(0.2457, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1000, 0, tensor(0.1758, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1020, 0, tensor(0.1588, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1040, 0, tensor(0.1229, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1060, 0, tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1080, 0, tensor(0.1613, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1100, 0, tensor(0.0971, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1120, 0, tensor(0.1977, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1140, 0, tensor(0.1284, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1160, 0, tensor(0.1276, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1180, 0, tensor(0.1185, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1200, 0, tensor(0.2048, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1220, 0, tensor(0.1665, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1240, 0, tensor(0.1649, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1260, 0, tensor(0.1331, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1280, 0, tensor(0.1467, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1300, 0, tensor(0.1827, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1320, 0, tensor(0.1948, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1340, 0, tensor(0.2054, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1360, 0, tensor(0.1874, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1380, 0, tensor(0.1947, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1400, 0, tensor(0.1079, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1420, 0, tensor(0.1004, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1440, 0, tensor(0.1169, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1460, 0, tensor(0.1077, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1480, 0, tensor(0.1040, device='cuda:0', grad_fn=<DivBackward0>))\n",
      "(1500, 0, tensor(0.1834, device='cuda:0', grad_fn=<DivBackward0>))\n"
     ]
    }
   ],
   "source": [
    "def main(stage,cuda=True):\n",
    "    if cuda and not torch.cuda.is_available():\n",
    "        raise Exception(\"No GPU found\")\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(123)\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "    \n",
    "    fg_path = '/scratch/matting/dataset/Training_set/adobe/fg/'\n",
    "    bg_path = '/scratch/matting/dataset/train2014/'\n",
    "    alpha_path = '/scratch/matting/dataset/Training_set/adobe/alpha/'\n",
    "    vgg_path = '/scratch/matting/dataset/vgg_state_dict.pth'\n",
    "    stage1_path = '/scratch/matting/dataset/full_trained.pth'\n",
    "    fg_file = '/scratch/matting/dataset/Training_set/training_fg_names.txt'\n",
    "    bg_file = '/scratch/matting/dataset/Training_set/training_bg_names.txt'\n",
    "    epochs = 10\n",
    "    files_list = get_files_list(fg_file, bg_file)\n",
    "    dataset = load_dataset(fg_path, alpha_path, bg_path, files_list)\n",
    "    model = get_model(stage)\n",
    "    print(model)\n",
    "#     vgg_dict = torch.load(vgg_path)\n",
    "#     model = copy_pretrain_vals(model, vgg_dict)\n",
    "    stage1_model = torch.load(stage1_path, map_location='cpu')['state_dict']\n",
    "    model = copy_pretrain_vals(model, stage1_model)\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "    for i in range(epochs):\n",
    "        train_model(stage ,model, optimizer, dataset, i, cuda)\n",
    "        save_model('/scratch/matting/model_' + str(i) + '_stage2.pth', model)\n",
    "main(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python opencv-fix",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
